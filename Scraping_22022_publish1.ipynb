{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORECASTING\n",
    "## SCRAPING PANEL DATA\n",
    "Start by defining the current date in the format required by websites for scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "20213\n",
      "2021-Q3\n",
      "202109\n",
      "2021-09\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    " \n",
    "current_date = datetime.now()\n",
    "current_quarter = round((current_date.month - 1) // 3 + 1)\n",
    "print(current_quarter)\n",
    "true_quarter=(current_quarter)\n",
    "if true_quarter>1:\n",
    "        this_date = datetime(current_date.year, true_quarter, 1)\n",
    "        this_date1 = datetime(current_date.year, current_quarter, 1)\n",
    "else:\n",
    "        this_date = datetime(current_date.year-1, true_quarter+8, 1)\n",
    "        this_date1 = datetime(current_date.year-1, current_quarter+2, 1) \n",
    "        start_date=datetime(2000,7,1)\n",
    "        end_date=this_date\n",
    "        num_months = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month)\n",
    "        num_quarters=(num_months+1)/3\n",
    "num_quarters=int(num_quarters)\n",
    "quarter = this_date.strftime(\"%Y%m\")\n",
    "quarter1 = this_date1.strftime('%Y%-m')\n",
    "quarter2 = this_date1.strftime('%Y-Q%-m')\n",
    "quarter3 = this_date1.strftime('%YQ%-m')\n",
    "quarter4= this_date.strftime(\"%Y-%m\")\n",
    "print(quarter1)\n",
    "print(quarter2)\n",
    "print(quarter)\n",
    "print(quarter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import all the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dictionary(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "         key_value = \"{0} : {1}\".format(key, value)\n",
    "    return key_value\n",
    "    #print(key_value)\n",
    "\n",
    "def heads(name,name1):\n",
    "    title=\"_\".join([name, name1])\n",
    "    return title\n",
    "\n",
    "def annual(values):\n",
    "    values_a=[]\n",
    "    delka=round((len(values)-2)/3)\n",
    "    for i in range(0,delka):\n",
    "        values_m=sum(values[i*3:i*3+3])/3\n",
    "        values_a.append(values_m)\n",
    "    return values_a\n",
    "\n",
    "def key_word(country,codes,name,frequency):\n",
    "    title=\".\".join([country, codes, name, frequency])\n",
    "    title=str(title)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, scrape data for the Czech variables PMI manufacturing, Interest rate 3-month PRIBOR, Nominal wage and Czech GDP: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape data for the Eurozone GDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data for all the EA19 countries and CZ\n",
    "\n",
    "#B1GQ #gdp\n",
    "#P31_S14_S15 #consumption of HHS and NPISH\n",
    "#PS_S13 #consumption of government\n",
    "#P51G #gross fixed capital formation\n",
    "#P5G #gross capital formation\n",
    "#P6 #exports\n",
    "#P7 #imports\n",
    "#D1 #compensation of employees\n",
    "\n",
    "head=[\"GDP\",\"CONS_HHS\", \"CONS_GOV\", \"THFK\", \"THK\", \"EXPORT\", \"IMPORT\"]\n",
    "head1={\"GDP\",\"CONS_HHS\", \"CONS_GOV\", \"THFK\", \"THK\", \"EXPORT\", \"IMPORT\"}\n",
    "countries=[\"EA19\",\"CZ\",\"BE\",\"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"LU\", \"NL\",\"AT\", \"PT\", \"FI\", \"MT\",\"SK\", \"EE\", \"LV\", \"LT\"]\n",
    "table=[]\n",
    "values=np.array([])\n",
    "queries=[\"B1GQ\", \"P31_S14_S15\", \"P3_S13\", \"P51G\", \"P5G\", \"P6\", \"P7\"]\n",
    "total_head=np.array([])\n",
    "#{quarter2}\n",
    "for country in countries:\n",
    "    if countries.index(country) >1:\n",
    "        dframe = pd.DataFrame(table,total_head)\n",
    "        dframe=dframe.T\n",
    "        dframe=dframe.dropna()\n",
    "        print(dframe)\n",
    "    if countries.index(country)%2 ==0 & countries.index(country)>1: \n",
    "        print(countries.index(country)%2)\n",
    "        print(countries.index(country))\n",
    "        time.sleep(60)\n",
    "    if countries.index(country) >8:\n",
    "        time.sleep(120)\n",
    "    for query in queries:\n",
    "        url = f'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/NAMQ_10_GDP/Q.CP_MEUR.SCA.{query}.{country}?startperiod=2000-Q3&endperiod={quarter2}&format=JSON&lang=en'\n",
    "        #wait for 3 second\n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text)\n",
    "        multiple_level_data = pd.json_normalize(data['value'])\n",
    "        df = pd.DataFrame.from_dict(multiple_level_data)\n",
    "        dff=df.T\n",
    "        \n",
    "        dff=dff.reset_index()\n",
    "        variable=dff[0]\n",
    "        variable=variable[(len(variable)-num_quarters):len(variable)]\n",
    "        variable=variable.reset_index(drop=True)\n",
    "        indexes = queries.index(query)\n",
    "        code=countries.index(country)\n",
    "        values=np.append(values,code)\n",
    "        total_name=heads(countries[code],head[indexes])\n",
    "        total_head=np.append(total_head,total_name)\n",
    "        a_dictionary={head[indexes] : variable}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        table.append(a_dictionary[head[indexes]])\n",
    "dframe = pd.DataFrame(table,total_head)\n",
    "dframe=dframe.T\n",
    "print(dframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "National Accounts Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          GDP  CONS_HHS  CONS_GOV    THFK     THK   EXPORT   IMPORT  values  \\\n",
      "1695  12685.0    7516.6    2308.0  2604.7  1470.3   9389.0   8248.3    1695   \n",
      "1696  12931.0    7484.9    2439.0  2853.4  1571.3   9811.4   8621.8    1696   \n",
      "1697  13382.0    7597.0    2348.7  2828.6  2019.7   9998.4   9032.8    1697   \n",
      "1698  13948.9    7718.9    2462.8  2978.5  2416.8  10312.8   9529.2    1698   \n",
      "1699  14553.8    8191.6    2537.7  2923.5  2950.8  11023.0  10754.1    1699   \n",
      "\n",
      "         GDP_EA  \n",
      "1695  2916162.5  \n",
      "1696  2927320.9  \n",
      "1697  2940044.8  \n",
      "1698  3010914.2  \n",
      "1699  3113705.2  \n"
     ]
    }
   ],
   "source": [
    "dframe=pd.read_csv('National_accounts_Eurostat.csv')\n",
    "full_series=[]\n",
    "line_new=[]\n",
    "dframe_new=[]\n",
    "head=[\"GDP\",\"CONS_HHS\", \"CONS_GOV\", \"THFK\", \"THK\", \"EXPORT\", \"IMPORT\"]\n",
    "head1={\"GDP\",\"CONS_HHS\", \"CONS_GOV\", \"THFK\", \"THK\", \"EXPORT\", \"IMPORT\"}\n",
    "countries=[\"CZ\",\"BE\",\"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"LU\", \"NL\",\"AT\", \"PT\", \"FI\", \"MT\",\"SK\", \"EE\", \"LV\", \"LT\"]\n",
    "for query in head:\n",
    "    indexes = head.index(query)\n",
    "    if indexes>0:\n",
    "        a_dictionary={head[indexes-1] : line_new}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        dframe_new.append(a_dictionary[head[indexes-1]])\n",
    "        line_new=[]\n",
    "    for country in countries:\n",
    "        indexes = head.index(query)\n",
    "        code=countries.index(country)\n",
    "        full_series1=heads(countries[code],head[indexes])\n",
    "        full_series=np.append(full_series,full_series1)\n",
    "        line=dframe[full_series1]\n",
    "        line_new=np.append(line_new,line,0)\n",
    "    if indexes==6:\n",
    "        a_dictionary={head[indexes-1] : line_new}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        dframe_new.append(a_dictionary[head[indexes-1]])\n",
    "        line_new=[]\n",
    "        \n",
    "dframe_newest=[]\n",
    "dframe_newest = pd.DataFrame(dframe_new,[\"GDP\",\"CONS_HHS\", \"CONS_GOV\", \"THFK\", \"THK\", \"EXPORT\", \"IMPORT\"]) #,full_series)\n",
    "dframe_newest=dframe_newest.T\n",
    "dframe_newest=dframe_newest\n",
    "val=list(range(0,len(dframe_newest)))\n",
    "dframe_newest['values']=val\n",
    "GDP_EA=[]\n",
    "length=int(len(dframe_newest)/len(dframe[\"CZ_GDP\"]))\n",
    "for i in range(0, length):\n",
    "    GDP_EAA=dframe[\"EA19_GDP\"]\n",
    "    GDP_EA=np.append(GDP_EA,GDP_EAA,0)\n",
    "dframe_newest['GDP_EA']=GDP_EA\n",
    "print(dframe_newest.tail(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data for all the EA19 countries and CZ, missing DE and FR - fill in EA19\n",
    "\n",
    "#D11 #Wages\n",
    "head=[\"WAGE\"]\n",
    "head1={\"WAGE\"}\n",
    "countries=[\"EA19\",\"CZ\",\"BE\", \"IE\", \"ES\", \"IT\",\"CY\",\"EL\", \"SI\", \"LU\", \"NL\",\"AT\", \"PT\", \"FI\", \"MT\",\"SK\", \"EE\", \"LV\", \"LT\"]#,\"DE\", \"FR\"]\n",
    "table=[]\n",
    "values=np.array([])\n",
    "queries=[\"D11\"]\n",
    "total_head=np.array([])\n",
    "#{quarter2}\n",
    "for country in countries:\n",
    "    if countries.index(country) >1:\n",
    "        dframe = pd.DataFrame(table,total_head)\n",
    "        dframe=dframe.T\n",
    "        dframe=dframe.dropna()\n",
    "        print(dframe)\n",
    "    if countries.index(country)%2 ==0 & countries.index(country)>1: \n",
    "        print(countries.index(country)%2)\n",
    "        print(countries.index(country))\n",
    "    for query in queries:\n",
    "        url = f'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/NAMQ_10_GDP/Q.CP_MEUR.SCA.{query}.{country}?startperiod=2000-Q3&endperiod={quarter2}&format=JSON&lang=en'\n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text)\n",
    "        multiple_level_data = pd.json_normalize(data['value'])\n",
    "        df = pd.DataFrame.from_dict(multiple_level_data)\n",
    "        dff=df.T\n",
    "        dff=dff.reset_index()\n",
    "        variable=dff[0]\n",
    "        variable=variable[len(variable)-85:len(variable)]\n",
    "        variable=variable.reset_index(drop=True)\n",
    "        indexes = queries.index(query)\n",
    "        code=countries.index(country)\n",
    "        values=np.append(values,code)\n",
    "        total_name=heads(countries[code],head[indexes])\n",
    "        total_head=np.append(total_head,total_name)\n",
    "        a_dictionary={head[indexes] : variable}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        table.append(a_dictionary[head[indexes]])\n",
    "dframe_w = pd.DataFrame(table,total_head)\n",
    "dframe_w=dframe_w.T\n",
    "dframe_w=dframe_w.dropna()\n",
    "dframe_w['DE_WAGE']=dframe_w['EA19_WAGE']\n",
    "dframe_w['FR_WAGE']=dframe_w['EA19_WAGE']\n",
    "print(dframe_w)\n",
    "#dframe_w.to_csv('Wages_Eurostat.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wage Panel appended to the Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_w=pd.read_csv('Wages_Eurostat.csv')\n",
    "full_series=[]\n",
    "line_new=[]\n",
    "dframe_new=[]\n",
    "head=[\"WAGE\"]\n",
    "head1={\"WAGE\"}\n",
    "countries=[\"CZ\",\"BE\", \"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"LU\", \"NL\",\"AT\", \"PT\", \"FI\", \"MT\",\"SK\", \"EE\", \"LV\", \"LT\"]\n",
    "\n",
    "for query in head:\n",
    "    indexes = head.index(query)\n",
    "    if indexes>0:\n",
    "        a_dictionary={head[indexes-1] : line_new}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        dframe_new.append(a_dictionary[head[indexes-1]])\n",
    "        line_new=[]\n",
    "    for country in countries:\n",
    "        indexes = head.index(query)\n",
    "        code=countries.index(country)\n",
    "        full_series1=heads(countries[code],head[indexes])\n",
    "        full_series=np.append(full_series,dframe_w[full_series1])\n",
    "        line=dframe_w[full_series1]\n",
    "        line_new=np.append(line_new,line,0)\n",
    "    if code==len(countries)-1:\n",
    "        a_dictionary={head[indexes-1] : line_new}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        dframe_new.append(a_dictionary[head[indexes-1]])\n",
    "        line_new=[]\n",
    "dframe_newest_w = pd.DataFrame(dframe_new,[\"WAGE\"]) #,full_series)\n",
    "dframe_newest_w=dframe_newest_w.T\n",
    "val=list(range(0,len(dframe_newest_w)))\n",
    "dframe_newest_w['values']=val\n",
    "\n",
    "#create EA wage variable\n",
    "WAGE_EA=[]\n",
    "length=int(len(dframe_newest_w)/len(dframe_w[\"CZ_WAGE\"]))\n",
    "for i in range(0, length):\n",
    "    WAGE_EAA=dframe_w[\"EA19_WAGE\"]\n",
    "    WAGE_EA=np.append(WAGE_EA,WAGE_EAA,0)\n",
    "dframe_newest_w['W_EA']=WAGE_EA\n",
    "dframe_total=pd.merge(dframe_newest,dframe_newest_w,on=\"values\")\n",
    "\n",
    "print(dframe_total.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_IR=[\"interest_rate\"]\n",
    "countries_IR=[\"EA\",\"CZ\"]\n",
    "table_IR=[]\n",
    "values_IR=[]\n",
    "total_head_IR=np.array([])\n",
    "for country in countries_IR:\n",
    "    indexes=countries_IR.index(country)\n",
    "    if indexes==0:\n",
    "        url=f'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/IRT_ST_Q/Q.IRT_M3.{country}?startperiod=2000-Q3&endperiod={quarter2}&format=JSON&lang=en'\n",
    "    if indexes==1:\n",
    "        url=f'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/IRT_ST_M/M.IRT_M3.CZ?startperiod=2000-M9&endperiod=2021-M9&format=JSON&lang=en'\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.text)\n",
    "    multiple_level_data = pd.json_normalize(data['value'])\n",
    "    df = pd.DataFrame.from_dict(multiple_level_data)\n",
    "    dff=df.T\n",
    "    dff=dff.reset_index()\n",
    "    variable=dff[0]\n",
    "    if indexes==0:\n",
    "        variable=variable[42:]\n",
    "        variable=variable.reset_index(drop=True)\n",
    "        \n",
    "    if indexes==1:\n",
    "        variable_m=variable[92:]\n",
    "        variable_m=variable_m.reset_index(drop=True)\n",
    "        variable=annual(variable_m)\n",
    "        variable=np.array(variable)\n",
    "    a_dictionary={countries_IR[indexes] : variable}\n",
    "    dic=print_dictionary(**a_dictionary)\n",
    "    values_IR.append(a_dictionary[countries_IR[indexes]])\n",
    "    total_name_IR=heads(countries_IR[indexes],head_IR[0])\n",
    "    total_head_IR=np.append(total_head_IR,total_name_IR)\n",
    "dframe_IR=pd.DataFrame(values_IR,total_head_IR)\n",
    "dframe_IR=dframe_IR.T\n",
    "dframe_IR=dframe_IR.dropna()\n",
    "print(dframe_IR)\n",
    "IR_CZ=dframe_IR['CZ_interest_rate']\n",
    "IR_EA=dframe_IR['EA_interest_rate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IR Panel appended to Main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dictionary(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "         key_value = \"{0} : {1}\".format(key, value)\n",
    "    return key_value\n",
    "    #print(key_value)\n",
    "\n",
    "def heads(name,name1):\n",
    "    title=\"_\".join([name, name1])\n",
    "    return title\n",
    "full_series=[]\n",
    "line_new=[]\n",
    "dframe_new=[]\n",
    "head=[\"IR\"]\n",
    "head1={\"IR\"}\n",
    "countries=[\"EA\"]\n",
    "line_new_cz=IR_CZ\n",
    "for query in range(1,20):\n",
    "    line_new_cz=np.append(line_new_cz,IR_EA,0)     \n",
    "dframe_total['IR']=line_new_cz\n",
    "dframe_total=dframe_total.loc[:,dframe_total.columns!='IR_EA']\n",
    "dframe_total=dframe_total.loc[:,dframe_total.columns!='IR_CZ']\n",
    "print(dframe_total.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eurostat Sentiment Indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "def print_dictionary(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "         key_value = \"{0} : {1}\".format(key, value)\n",
    "    return key_value\n",
    "\n",
    "def heads(name,name1):\n",
    "    title=\"_\".join([name, name1])\n",
    "    return title\n",
    "\n",
    "head=[\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"]\n",
    "head1={\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"}\n",
    "countries=[\"EA19\",\"CZ\",\"BE\",\"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"NL\",\"AT\", \"PT\", \"FI\", \"SK\", \"EE\"] #Missing countries LU LT LV MT\n",
    "table=[]\n",
    "dframe_sen1=[]\n",
    "values=np.array([])\n",
    "queries=[\"BS-CCI-BAL\",\"BS-CSMCI-BAL\",\"BS-ICI-BAL\",\"BS-RCI-BAL\",\"BS-SCI-BAL\"]\n",
    "total_head=np.array([])\n",
    "for country in countries:\n",
    "    if countries.index(country) >1:\n",
    "        dframe_sen1 = pd.DataFrame(table,total_head)\n",
    "        dframe_sen1=dframe_sen1.T\n",
    "        dframe_sen1=dframe_sen1.dropna()\n",
    "    if countries.index(country)%2 ==0 & countries.index(country)>1: \n",
    "        time.sleep(30)\n",
    "    if countries.index(country) >8:\n",
    "        time.sleep(30)\n",
    "    for query in queries:\n",
    "        url = f'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/EI_BSSI_M_R2/M.{query}.SA.{country}?startperiod=2000-01&endperiod={quarter4}&format=JSON&lang=en'\n",
    "    \n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text)\n",
    "        multiple_level_data = pd.json_normalize(data['value'])\n",
    "        df = pd.DataFrame.from_dict(multiple_level_data)\n",
    "        dff=df.T\n",
    "        variable=dff[0]\n",
    "        variable=variable[len(variable)-259:len(variable)-4]\n",
    "        variable=variable.reset_index(drop=True)\n",
    "        var_ind=variable.index\n",
    "        indexes = queries.index(query)\n",
    "        code=countries.index(country)\n",
    "        values=np.append(values,code)\n",
    "        total_name=heads(countries[code],head[indexes])\n",
    "        total_head=np.append(total_head,total_name)\n",
    "        a_dictionary={head[indexes] : variable}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        table.append(a_dictionary[head[indexes]])\n",
    "dframe_sen1 = pd.DataFrame(table,total_head)\n",
    "dframe_sen1=dframe_sen1.reset_index(drop=True)\n",
    "dframe_sen1=dframe_sen1.T\n",
    "\n",
    "print(dframe_sen1)\n",
    "\n",
    "#df=dframe_sen1\n",
    "#df=df.interpolate(method='linear', limit_direction='forward')\n",
    "#df=df.interpolate(method='linear', limit_direction='backward',limit=1)\n",
    "#df.to_csv('Eurostat_sensitivity.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity - dataset manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dframe_sen1\n",
    "df=df.interpolate(method='linear', limit_direction='forward')\n",
    "df=df.interpolate(method='linear', limit_direction='backward',limit=1)\n",
    "print(df)\n",
    "df.to_csv('Eurostat_sensitivity.csv', index=False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def heads(name,name1):\n",
    "    title=\"_\".join([name, name1])\n",
    "    return title\n",
    "\n",
    "dframe_sen=pd.read_csv('Eurostat_sensitivity.csv',header=[0])\n",
    "indicators=[\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"]\n",
    "countries=[\"EA19\",\"CZ\",\"BE\",\"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"NL\",\"AT\", \"PT\", \"FI\", \"SK\", \"EE\"]\n",
    "label_total=[]\n",
    "label=[]\n",
    "dframe_sensitivity=[]\n",
    "for query in indicators:\n",
    "    for country in countries:\n",
    "        label=heads(country,query)\n",
    "        label_total.append(label)\n",
    "print(len(label_total))\n",
    "dframe_sen=pd.DataFrame(dframe_sen)\n",
    "dframe_sen.columns=label_total\n",
    "\n",
    "dframe_sen.to_csv('Eurostat_sensitivity_total.csv')\n",
    "val=list(range(0,len(dframe_sen)))\n",
    "dframe_sen['values']=val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Total Sentiment Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_sen=pd.read_csv('Eurostat_sensitivity_total.csv')\n",
    "\n",
    "w=[0.05,0.2,0.4,0.05,0.3]\n",
    "indicators=[\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"]\n",
    "countries=[\"EA19\",\"CZ\",\"BE\",\"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"NL\",\"AT\", \"PT\", \"FI\", \"SK\", \"EE\"]\n",
    "table=[]\n",
    "table_total=[]\n",
    "label_matrix=[]\n",
    "title_tot=[]\n",
    "def heads(name,name1):\n",
    "    title=\"_\".join([name, name1])\n",
    "    return title\n",
    "for country in countries:\n",
    "    country_index=(countries.index(country))\n",
    "\n",
    "    for i in indicators:\n",
    "        indexes = indicators.index(i)\n",
    "        title=heads(country,i)\n",
    "        title_tot.append(title)\n",
    "        table.append(dframe_sen[title])\n",
    "    label='TOTAL_SENTIMENT'\n",
    "    label_total=heads(country,label)\n",
    "    table_country=pd.DataFrame(table)\n",
    "    table_country=table_country.T\n",
    "    label_matrix.append(label_total)\n",
    "    total=(0.05*table_country[title_tot[(country_index)]]+0.2*table_country[title_tot[(country_index+1)]]+0.4*table_country[title_tot[(country_index+2)]]+0.05*table_country[title_tot[(country_index+3)]]+0.3*table_country[title_tot[(country_index+4)]])\n",
    "    table_total.append(total)\n",
    "TOTAL_SEN=pd.DataFrame(table_total,label_matrix)\n",
    "TOTAL_SEN=TOTAL_SEN.T\n",
    "val=list(range(0,len(dframe_sen)))\n",
    "TOTAL_SEN['values']=val\n",
    "dframe_sen_total=pd.merge(TOTAL_SEN,dframe_sen,on=\"values\")\n",
    "print(dframe_sen_total)\n",
    "dframe_sen_total.to_csv('Eurostat_sen_total.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Indices - aggregate from Monthly to Quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual(values):\n",
    "    values_a=[]\n",
    "    delka=round((len(values)-1)/3)\n",
    "    for i in range(0,delka):\n",
    "        values_m=sum(values[i*3:i*3+3])/3\n",
    "        values_a.append(values_m)\n",
    "    return values_a\n",
    "\n",
    "dframe_sen_total=pd.read_csv('Eurostat_sen_total.csv')\n",
    "\n",
    "indicators=[\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"]\n",
    "countries=[\"EA19\",\"CZ\",\"BE\",\"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"NL\",\"AT\", \"PT\", \"FI\", \"SK\", \"EE\"]\n",
    "countries_fill=[\"LU\",\"MT\",\"LV\", \"LT\"]\n",
    "label_total=[]\n",
    "label_missing=[]\n",
    "value_missing=[]\n",
    "label=[]\n",
    "dframe_sensitivity=[]\n",
    "dframe_sen_annual=[]\n",
    "for query in indicators:\n",
    "    for country in countries:\n",
    "        label=heads(country,query)\n",
    "        label_total.append(label)       \n",
    "\n",
    "for name in label_total:\n",
    "    variable_m=dframe_sen_total[name]\n",
    "    variable=annual(variable_m)\n",
    "    dframe_sensitivity.append(variable)\n",
    "\n",
    "dframe_sen_annual=pd.DataFrame(dframe_sensitivity,label_total)\n",
    "dframe_sen_annual=dframe_sen_annual.T\n",
    "\n",
    "for query in indicators:\n",
    "    for country in countries_fill:\n",
    "        label=heads(country,query)\n",
    "        label1=heads(\"EA19\",query)\n",
    "        label_missing.append(label)\n",
    "        value_missing.append(dframe_sen_annual[label1])\n",
    "dframe_sen_filled=pd.DataFrame(value_missing,label_missing)\n",
    "dframe_sen_filled=dframe_sen_filled.T\n",
    "\n",
    "val=list(range(0,len(dframe_sen_annual)))\n",
    "dframe_sen_annual['values']=val\n",
    "dframe_sen_filled['values']=val\n",
    "dframe_sen_total=pd.merge(dframe_sen_annual,dframe_sen_filled,on=\"values\")\n",
    "dframe_newest=pd.merge(dframe_newest,dframe_sen_total,on=\"values\")\n",
    "print(dframe_sen_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_sens=dframe_sen_total\n",
    "full_series=[]\n",
    "line_new=[]\n",
    "dframe_new=[]\n",
    "head=[\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"]\n",
    "head1={\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"}\n",
    "head2=[\"CONST_INDEX\",\"CON_INDEX\", \"IND_INDEX\", \"RETAIL_INDEX\", \"SERV_INDEX\"]\n",
    "\n",
    "countries=[\"CZ\",\"BE\", \"DE\", \"IE\", \"ES\", \"FR\", \"IT\",\"CY\",\"EL\", \"SI\", \"LU\", \"NL\",\"AT\", \"PT\", \"FI\", \"MT\",\"SK\", \"EE\", \"LV\", \"LT\"] #\"EA19\"\n",
    "\n",
    "for query in head:\n",
    "    indexes = head.index(query)\n",
    "    for country in countries:\n",
    "        indexes = head.index(query)\n",
    "        code=countries.index(country)\n",
    "        full_series1=heads(countries[code],head[indexes])\n",
    "        full_series=np.append(full_series,dframe_sens[full_series1])\n",
    "        line=dframe_sens[full_series1]\n",
    "        line_new=np.append(line_new,line,0)\n",
    "\n",
    "    if code==len(countries)-1:\n",
    "        a_dictionary={head[indexes-1] : line_new}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        dframe_new.append(a_dictionary[head[indexes-1]])\n",
    "        line_new=[]\n",
    "\n",
    "dframe_newest_sen = pd.DataFrame(dframe_new,head2) #,full_series)\n",
    "dframe_newest_sen=dframe_newest_sen.T\n",
    "val=list(range(0,len(dframe_newest_sen)))\n",
    "dframe_newest_sen['values']=val\n",
    "dframe_all=pd.merge(dframe_total,dframe_newest_sen,on=\"values\")\n",
    "\n",
    "dframe_print=dframe_all.loc[:,dframe_all.columns!= 'values']\n",
    "#dframe_print.to_csv('Database_panel.csv')\n",
    "print(dframe_print.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the VAR models, scrape OECD data for the variables: Eurozone interest rate, Eurozne HICP, Czech HIC and Czech Employment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pandasdmx import Request\n",
    "oecd = Request('OECD')\n",
    "values=[]\n",
    "total_head=[]\n",
    "table=[]\n",
    "countries=[\"EA19\", \"CZE\", \"BEL\", \"DEU\", \"IRL\", \"ESP\", \"FRA\", \"ITA\", \"GRC\", \"SVN\", \"LUX\", \"NLD\", \"AUT\", \"PRT\", \"FIN\", \"SVK\", \"EST\", \"LVA\", \"LTU\"] #NO CY MT \"EA19\"\n",
    "queries=[\"L\",\"CPI\"]\n",
    "for country in countries:  \n",
    "    code=countries.index(country)\n",
    "    if countries.index(country) >1:\n",
    "        dframe_oecd = pd.DataFrame(table,total_head)\n",
    "        dframe_oecd=dframe_oecd.T\n",
    "        dframe_oecd=dframe_oecd.dropna()  \n",
    "    for query in queries:\n",
    "        indexes = queries.index(query)\n",
    "        if indexes==0 & code > 0:\n",
    "            try: #EMPLOYMENT\n",
    "                keys=key_word(countries[code],\"ETONC\",\"PERSA\",\"Q\")\n",
    "                data_response = oecd.data(resource_id='QNA', key=keys, params={'startPeriod': '2000Q3', 'endPeriod':{quarter3}})\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            except KeyError:\n",
    "                pass\n",
    "            else:\n",
    "                oecd_data = data_response.data\n",
    "\n",
    "                df = data_response.write(oecd_data.series, parse_time='True')\n",
    "                df = data_response.write().unstack().reset_index()\n",
    "                df_L=pd.DataFrame(df)\n",
    "                df_L = df_L.sort_values([\"TIME_PERIOD\"], ascending = (True))\n",
    "                variable=df_L.iloc[:,5].values\n",
    "                time=df_L.iloc[:,4].values\n",
    "        if indexes==1:\n",
    "\n",
    "            try: #CPI\n",
    "                keys=key_word(countries[code],\"CPHPTT01\",\"IXOB\",\"M\") \n",
    "                data_response = oecd.data(resource_id='PRICES_CPI', key=keys, params={'startPeriod': '2000-07', 'endPeriod':{quarter4}})\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            except KeyError:\n",
    "                pass\n",
    "            else:\n",
    "                oecd_data = data_response.data\n",
    "\n",
    "                df = data_response.write(oecd_data.series, parse_time='True')\n",
    "                df = data_response.write().unstack().reset_index()\n",
    "                df_var=pd.DataFrame(df)\n",
    "                df_var = df_var.sort_values([\"TIME_PERIOD\"], ascending = (True))\n",
    "                var_m=df_var.iloc[:,5].values\n",
    "                gg=len(var_m)\n",
    "                var=[0]*gg\n",
    "                for m in range(0,(len(var)-1)):\n",
    "                    var[m]=sum(var_m[(3*m):(2+3*m)])/3\n",
    "                variable=var[0:len(df_L.iloc[:,5].values)]\n",
    "        indexes = queries.index(query)\n",
    "        code=countries.index(country)\n",
    "        values=np.append(values,code)\n",
    "        total_name=heads(countries[code],queries[indexes])\n",
    "        total_head=np.append(total_head,total_name)\n",
    "        a_dictionary={queries[indexes] : variable}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        table.append(a_dictionary[queries[indexes]])\n",
    "\n",
    "EA_CPI=[]\n",
    "df_oecd=pd.DataFrame(data=table)\n",
    "dframe_oecd = pd.DataFrame(table,total_head)\n",
    "dframe_oecd=dframe_oecd.T\n",
    "EA_CPI=[]\n",
    "for country in countries:\n",
    "    code=countries.index(country)\n",
    "    if code>0:\n",
    "        name=heads(country,\"L\")\n",
    "        EA_CPI=sum(EA_CPI,dframe_oecd[name]) \n",
    "dframe_oecd['EA_CPI']=EA_CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_oecd['EA19_CPI']=dframe_oecd['EA_CPI']\n",
    "dframe_oecd1=dframe_oecd\n",
    "\n",
    "full_series=[]\n",
    "line_new=[]\n",
    "dframe_new=[]\n",
    "line=[]\n",
    "head=[\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"]\n",
    "head1={\"const_conf\", \"consumer_conf\", \"indust_conf\", \"retail_conf\", \"serv_conf\"}\n",
    "head2=[\"CONST_INDEX\",\"CON_INDEX\", \"IND_INDEX\", \"RETAIL_INDEX\", \"SERV_INDEX\"]\n",
    "\n",
    "#Missing countries CY MT replaced by EA19\n",
    "countries1=[\"CZE\",\"BEL\", \"DEU\", \"IRL\", \"ESP\", \"FRA\", \"ITA\", \"EA19\", \"GRC\", \"SVN\", \"LUX\", \"NLD\", \"AUT\", \"PRT\", \"FIN\", \"EA19\",\"SVK\", \"EST\", \"LVA\", \"LTU\"] \n",
    "head=[\"L\",\"CPI\"]\n",
    "\n",
    "for query in head:\n",
    "    indexes = head.index(query)\n",
    "    for country in countries1:\n",
    "        indexes = head.index(query)\n",
    "        code=countries1.index(country)\n",
    "        full_series1=heads(countries1[code],head[indexes])\n",
    "        full_series=np.append(full_series,dframe_oecd1[full_series1]) \n",
    "        line=dframe_oecd1[full_series1]\n",
    "        line_new=np.append(line_new,line,0)\n",
    "\n",
    "    if code==len(countries1)-1:\n",
    "        a_dictionary={head[indexes-1] : line_new}\n",
    "        dic=print_dictionary(**a_dictionary)\n",
    "        dframe_new.append(a_dictionary[head[indexes-1]])\n",
    "        line_new=[]\n",
    "\n",
    "dframe_new_oecd = pd.DataFrame(dframe_new,head)\n",
    "dframe_new_oecd=dframe_new_oecd.T\n",
    "val=list(range(0,len(dframe_new_oecd)))\n",
    "dframe_new_oecd['values']=val\n",
    "dframe_var=pd.merge(dframe_all,dframe_new_oecd,on=\"values\")\n",
    "\n",
    "print(dframe_var.head(5))\n",
    "dframe_var.to_csv('Database_panel_var.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CZK     USD\n",
      "0   35.465  0.9052\n",
      "1   34.896  0.8683\n",
      "2   34.791  0.9232\n",
      "3   34.288  0.8725\n",
      "4   34.021  0.8903\n",
      "..     ...     ...\n",
      "80  26.479  1.1689\n",
      "81  26.667  1.1929\n",
      "82  26.070  1.2048\n",
      "83  25.638  1.2058\n",
      "84  25.500  1.1788\n",
      "\n",
      "[85 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "countries=[\"CZK\",\"USD\"]\n",
    "table=[]\n",
    "values=np.array([])\n",
    "total_head=np.array([])\n",
    "for country in countries:\n",
    "    if countries.index(country) >1:\n",
    "        dframe = pd.DataFrame(table,total_head)\n",
    "        dframe=dframe.T\n",
    "        dframe=dframe.dropna()\n",
    "\n",
    "    url = f'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/ERT_BIL_EUR_Q/Q.AVG.NAC.{country}?startperiod=2000-09&endperiod={quarter2}&format=JSON&lang=en'\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.text)\n",
    "    multiple_level_data = pd.json_normalize(data['value'])\n",
    "    df = pd.DataFrame.from_dict(multiple_level_data)\n",
    "    dff=df.T\n",
    "    dff=dff.reset_index()\n",
    "    variable=dff[0]\n",
    "    variable=variable[len(variable)-86:len(variable)-1]\n",
    "    variable=variable.reset_index(drop=True)\n",
    "    code=countries.index(country)\n",
    "    values=np.append(values,code)\n",
    "    total_name=countries\n",
    "    total_head=countries\n",
    "    a_dictionary={countries[code] : variable}\n",
    "    dic=print_dictionary(**a_dictionary)\n",
    "    table.append(a_dictionary[countries[code]])\n",
    "dframe_NER = pd.DataFrame(table,total_head)\n",
    "dframe_NER=dframe_NER.T\n",
    "dframe_NER=dframe_NER.dropna()\n",
    "print(dframe_NER)\n",
    "\n",
    "label_total=[]\n",
    "label=[]\n",
    "dframe_sensitivity=[]\n",
    "for repeat in range(1,15):\n",
    "    label_total.append(dframe_NER['USD'])\n",
    "label_total.append(dframe_NER['CZK'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_total=[]\n",
    "label_total_ner=[]\n",
    "for repeat in range(1,20):\n",
    "    label_total=np.append(label_total,dframe_NER['USD'])\n",
    "label_total_ner=np.append(dframe_NER['CZK'],label_total)\n",
    "dframe_var['NER']=label_total_ner\n",
    "print(dframe_var.head(5))\n",
    "dframe_var.to_csv('GDP_VAR_abs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET YOY\n",
    "Calculate the final dataset in year-over-year growth rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columm_all=[]\n",
    "columm=pd.date_range(start=\"2001-09-01\",end=\"2021-09-01\", freq='Q')\n",
    "colummq=columm.strftime(\"%Y%m\").tolist()\n",
    "\n",
    "for i in range(0,21):\n",
    "    columm_all=np.append(columm_all,colummq) \n",
    "\n",
    "column_names=dframe_var.columns\n",
    "dYoY= [columm]\n",
    "YoY1=[]\n",
    "dYoY1= pd.DataFrame()\n",
    "YoY=[]\n",
    "\n",
    "for var in range(0,20):\n",
    "    YoY1=[]\n",
    "    for block in range(0,20):\n",
    "        start_values=block*85\n",
    "        end_values=start_values+84\n",
    "        if block==20:\n",
    "            end_values=start_values+85\n",
    "        QoQ=dframe_var.iloc[start_values:end_values,var]\n",
    "        QoQ4=QoQ.shift(-4)\n",
    "        QoQ=pd.to_numeric(QoQ, errors='coerce')\n",
    "        QoQ4=pd.to_numeric(QoQ4, errors='coerce')\n",
    "        YoY=(QoQ4/QoQ-1)*100        \n",
    "        YoY1=np.append(YoY1,YoY)\n",
    "    dYoY1.loc[:,var]=YoY1\n",
    "dYoY1.columns=column_names\n",
    "dYoY1=dYoY1.loc[:,dYoY1.columns!='values']\n",
    "column_indices=[\"CONST_INDEX\",\"CON_INDEX\",\"IND_INDEX\",\"RETAIL_INDEX\",\"SERV_INDEX\",\"IR\"]\n",
    "\n",
    "for col in column_indices:\n",
    "    variable=[]\n",
    "    dYoY1=dYoY1.loc[:,dYoY1.columns!=col]\n",
    "    variable=dframe_var.loc[:,col]\n",
    "    variable=variable.shift(-4)\n",
    "    dYoY1[col]=variable\n",
    "dYoY1[\"date\"]=columm_all\n",
    "first_col=dYoY1.pop('date')\n",
    "dYoY1.insert(0,'date',first_col)\n",
    "dYoY1=dYoY1.dropna()\n",
    "dYoY1=dYoY1.loc[:,dYoY1.columns!='date']\n",
    "print(dYoY1)\n",
    "\n",
    "dYoY1.to_csv(\"Database_rates.csv\")\n",
    "#dframe_print=dframe_all.loc[:,dframe_all.columns!= 'values']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add extra columns for simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database=pd.read_csv('Database_rates.csv')\n",
    "# Panel data index\n",
    "indexes=[]\n",
    "for values in range(1,21):\n",
    "    ind=np.repeat(values,80).tolist()\n",
    "    indexes=np.append(indexes,ind)\n",
    "database=database.iloc[:,1:]\n",
    "database['index']=indexes\n",
    "first_col=database.pop('index')\n",
    "database.insert(0,'index',first_col)\n",
    "\n",
    "# Date\n",
    "dates=pd.to_datetime(database[\"date\"],format='%Y%m')\n",
    "year=dates.dt.year\n",
    "month=dates.dt.month\n",
    "database[\"YEAR\"]=year\n",
    "col1=database.pop(\"YEAR\")\n",
    "database[\"MONTH\"]=month\n",
    "col2=database.pop(\"MONTH\")\n",
    "val=list(np.repeat(1,len(database)))\n",
    "database.insert(loc=0,column='month',value=month)\n",
    "database.insert(loc=1,column='year',value=year)\n",
    "database.insert(loc=2,column='DAY',value=val)\n",
    "database.insert(3, \"MONTH\", col2)\n",
    "database.insert(4, \"YEAR\", col1)\n",
    "database=database.loc[:,database.columns!='date']\n",
    "\n",
    "#NX\n",
    "NX=database.loc[:,'EXPORT']-database.loc[:,'IMPORT']\n",
    "database['NX']=NX\n",
    "print(database.head(5))\n",
    "database.to_csv(\"Database_rates_new.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lagged explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# load data\n",
    "def parse(x):\n",
    "\treturn datetime.strptime(x, '%Y %m') #to consolidate date so we can use it in pandas\n",
    "dataset = read_csv('Database_rates_new.csv',  parse_dates = [['year', 'month']], index_col=0, date_parser=parse)\n",
    "dataset.index.name = 'date'\n",
    "dataset.to_csv('Database_rates_new1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "# load dataset\n",
    "dataset = read_csv('Database_rates_new1.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# specify columns to plot\n",
    "groups = [1, 2, 3, 4, 5, 6]\n",
    "i = 1\n",
    "# plot each column\n",
    "pyplot.figure()\n",
    "for group in groups:\n",
    "\tpyplot.subplot(len(groups), 1, i)\n",
    "\tpyplot.plot(values[:, group])\n",
    "\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n",
    "\ti += 1\n",
    "pyplot.show()\n",
    " \n",
    "# convert series\n",
    "# for the fce see https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): #n_in no of lag observations you want to create as columns, n_out no of observations as output\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tdf_block=pd.DataFrame()\n",
    "\tfor i in range(n_in, 0, -1): #n_in number of lag observations as input (X)\n",
    "\t\tfor values in range(0,len(df)):\n",
    "\t\t\tstart=int(values*80)\n",
    "\t\t\tend=int(start+80)\n",
    "\t\t\tdf_sample=df.iloc[start:end,:]\n",
    "\t\t\tdf_shifted=df_sample.shift(i) #shift series and create new column for every shifted series\n",
    "\t\t\tdf_block=pd.concat([df_block,df_shifted])\n",
    "\t\tcols.append(df_block)\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tdf_sample=[]\n",
    "\tdf_shifted=[]\n",
    "\tdf_block=pd.DataFrame()\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tfor values in range(0,len(df)):\n",
    "\t\t\tstart=int(values*80)\n",
    "\t\t\tend=int(start+80)\n",
    "\t\t\tdf_sample=df.iloc[start:end,:]\n",
    "\t\t\tdf_shifted=df_sample.shift(-i)\n",
    "\t\t\tdf_block=pd.concat([df_block,df_shifted])\n",
    "\t\tcols.append(df_block) #[put series forward for forecasting]\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv('Database_rates_new1.csv', header=0, index_col=0)\n",
    "dataset=pd.DataFrame(dataset)\n",
    "\n",
    "indexes=[]\n",
    "time_total=[]\n",
    "for values in range(1,22):\n",
    "    ind=np.repeat(values,80).tolist()\n",
    "    time=list(range(1,80))\n",
    "    time_total=np.append(time_total,time)\n",
    "    indexes=np.append(indexes,ind)\n",
    "indexes=indexes[0:len(dataset)]\n",
    "time_total=time_total[0:len(dataset)]\n",
    "dataset['index']=indexes\n",
    "dataset['time']=time_total\n",
    "first_col=dataset.pop('index')\n",
    "dataset.insert(0,'index',first_col)\n",
    "first_col=dataset.pop('time')\n",
    "dataset.insert(1,'time',first_col)\n",
    "\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "#encoder = LabelEncoder()\n",
    "#values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1) #thats the fce in the beginning, saying to create extra column for shifted series and also for forecasted series\n",
    "#reframed=pd.DataFrame(reframed)\n",
    "# drop columns we don't want to predict\n",
    "#reframed.drop(reframed.columns[1], axis=1, inplace=True)\n",
    "\n",
    "reframed=pd.DataFrame(reframed)\n",
    "print(reframed.head())\n",
    "reframed.to_csv('reframed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add line with 1.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_all=[]\n",
    "for values in reframed.iloc[:,0]:\n",
    "    if values==0:\n",
    "        values_all=np.append(values_all,values)\n",
    "czech_end=(len(values_all))\n",
    "\n",
    "reframed1=reframed\n",
    "length=len(reframed1)\n",
    "data=[]\n",
    "no_col=int(len(reframed1.columns)/2)\n",
    "no_col_full=no_col*2\n",
    "final=[]\n",
    "for col in range(0,no_col):\n",
    "    value=reframed1.iloc[czech_end-1,col+25]\n",
    "    data=np.append(data,value)\n",
    "print(data)\n",
    "for col in range(no_col,no_col_full):\n",
    "    data=np.append(data,1.02)\n",
    "dataset=pd.DataFrame(data)\n",
    "dataset=dataset.T\n",
    "print(dataset)\n",
    "dataset.columns=reframed1.columns\n",
    "final=reframed1.append(dataset,ignore_index=True)\n",
    "print(final.tail(5))\n",
    "final.to_csv('final.csv')\n",
    "final.to_csv('final_xgb.csv')\n",
    "final.to_csv('final_rf.csv')\n",
    "final.to_csv('final_xgb_exo.csv')\n",
    "final.to_csv('final_rf_exo.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING PART\n",
    "### RANDOM FOREST AND XGBOOST MODELS\n",
    "\n",
    "This section estimates 200 rf/xgb models and selects those with the highest R-squared and lowest rmse to derive the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import numpy as geek\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Select model\n",
    "model='rf' #or 'xgb' or 'LSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly split data to test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevengawthorpe/.pyenv/versions/my-env/lib/python3.8/site-packages/pandas/core/indexes/base.py:3941: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('final.csv') #GDP_tree_VARYoY_Yrf01.csv\n",
    "\n",
    "val=list(range(0,no_col))\n",
    "column_indices = [val]\n",
    "new_names = ['index','index','date', 'day',\t'month', 'year',\t'GDP',\t'CONS_HHS',\t'CONS_GOV',\t'THFK',\t'THK',\t'EXPORT', 'IMPORT',\t'GDP_EA',\t'WAGE',\t'W_EA',\t'L', 'CPI', 'NER', 'CONST_INDEX', 'CON_INDEX', 'IND_INDEX', 'RETAIL_INDEX',\t'SERV_INDEX', 'IR', 'NX']\n",
    "old_names = dataset.columns[column_indices]\n",
    "dataset.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "#get vector of quarters and years to add to the new prediction line\n",
    "\n",
    "#select values for the variables used in the model\n",
    "hh=dataset.head()\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_size=0.8 #0.8\n",
    "train=pd.DataFrame()\n",
    "test=pd.DataFrame()\n",
    "\n",
    "import random\n",
    "random_train=[]\n",
    "random_test=[]\n",
    "train_bulk=[]\n",
    "test_bulk=[]\n",
    "\n",
    "train_value=int(80*train_size)\n",
    "train_bulk=random.sample(range(79), train_value)\n",
    "train_bulk=np.sort(train_bulk)\n",
    "vector=list(range(0,80))\n",
    "\n",
    "for j in range(0,79):\n",
    "    if j not in train_bulk:\n",
    "        test_bulk.append(j)\n",
    "celek=int(len(dataset)/80)+1\n",
    "for val in range(0,celek):\n",
    "    val=int(val)\n",
    "    vall=val*80+80\n",
    "    if vall<len(dataset):\n",
    "        for v in train_bulk:\n",
    "            train_sample=dataset.iloc[v+79*val,:]\n",
    "            train_sample=pd.DataFrame(train_sample)\n",
    "            train_sample=train_sample.T\n",
    "            train=train.append(train_sample)\n",
    "            #train=pd.concat([train,train_sample])\n",
    "        for h in test_bulk:\n",
    "            test_sample=dataset.iloc[h+79*val,:]\n",
    "            test_sample=pd.DataFrame(test_sample)\n",
    "            test_sample=test_sample.T\n",
    "            test=test.append(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021, 2021, 2022, 2022, 2022, 2022, 2023, 2023, 2023, 2023, 2024, 2024, 2024, 2024, 2025, 2025, 2025, 2025]\n",
      "[ 9 12  3  6  9 12  3  6  9 12  3  6  9 12  3  6  9 12]\n"
     ]
    }
   ],
   "source": [
    "quarter=[]\n",
    "years=[]\n",
    "for k in range(19):\n",
    "    quarter=(2021+k)\n",
    "    for i in range(4):\n",
    "        years.append(quarter)\n",
    "import numpy\n",
    "year=years[2:20]\n",
    "months=[]\n",
    "quarters=[0]\n",
    "for k in range(4):\n",
    "    quarter=((k+1)*3)\n",
    "    months.append(quarter)\n",
    "month=numpy.tile(months,19)\n",
    "month=month[2:len(year)+2]\n",
    "print(year)\n",
    "print(month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='xgb'\n",
    "\n",
    "dataset = pd.read_csv('final.csv')\n",
    "dataset_names = pd.read_csv('final.csv')\n",
    "no_col=int(len(dataset.columns)/2)\n",
    "\n",
    "no_col_full=no_col*2\n",
    "matrix = geek.zeros([no_col_full, no_col_full])\n",
    "matrix_row = pd.Series\n",
    "vector_zeros=[0]*300\n",
    "delka=len(vector_zeros)\n",
    "\n",
    "values_all=[]\n",
    "for values in reframed.iloc[:,0]:\n",
    "    if values==0:\n",
    "        values_all=np.append(values_all,values)\n",
    "czech_end=(len(values_all))\n",
    "\n",
    "result_endo_pred=pd.DataFrame()\n",
    "dataframe_endo=pd.DataFrame()\n",
    "result_endo_pred1=pd.DataFrame()\n",
    "dataframe_endo=pd.DataFrame()\n",
    "values=pd.DataFrame()\n",
    "values_all=pd.DataFrame()\n",
    "\n",
    "result=0\n",
    "result_total=np.zeros(no_col-5)\n",
    "\n",
    "for time in range(1,11): #20\n",
    "    #Code for random split\n",
    "    dataset = pd.read_csv('final.csv')\n",
    "    val=list(range(0,no_col))\n",
    "    column_indices = [val]\n",
    "    new_names = ['index','index','date', 'day',\t'month', 'year',\t'GDP',\t'CONS_HHS',\t'CONS_GOV',\t'THFK',\t'THK',\t'EXPORT', 'IMPORT',\t'GDP_EA',\t'WAGE',\t'W_EA',\t'L', 'CPI', 'NER', 'CONST_INDEX', 'CON_INDEX', 'IND_INDEX', 'RETAIL_INDEX',\t'SERV_INDEX', 'IR', 'NX']\n",
    "    old_names = dataset.columns[column_indices]\n",
    "    dataset.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "    hh=dataset.head()\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_size=0.8 #0.8\n",
    "    train=pd.DataFrame()\n",
    "    test=pd.DataFrame()\n",
    "\n",
    "    import random\n",
    "    random_train=[]\n",
    "    random_test=[]\n",
    "    train_bulk=[]\n",
    "    test_bulk=[]\n",
    "\n",
    "    train_value=int(80*train_size)\n",
    "    train_bulk=random.sample(range(79), train_value)\n",
    "    train_bulk=np.sort(train_bulk)\n",
    "    vector=list(range(0,80))\n",
    "\n",
    "    for j in range(0,79):\n",
    "        if j not in train_bulk:\n",
    "            test_bulk.append(j)\n",
    "    celek=int(len(dataset)/80)+1\n",
    "    for val in range(0,celek):\n",
    "        val=int(val)\n",
    "        vall=val*80+80\n",
    "        if vall<len(dataset):\n",
    "            for v in train_bulk:\n",
    "                train_sample=dataset.iloc[v+79*val,:]\n",
    "                train_sample=pd.DataFrame(train_sample)\n",
    "                train_sample=train_sample.T\n",
    "                train=train.append(train_sample)\n",
    "            for h in test_bulk:\n",
    "                test_sample=dataset.iloc[h+79*val,:]\n",
    "                test_sample=pd.DataFrame(test_sample)\n",
    "                test_sample=test_sample.T\n",
    "                test=test.append(test_sample)\n",
    "    #end of random split\n",
    "\n",
    "    for x in range(1,11):#21\n",
    "\n",
    "        scenario=x\n",
    "        sum_result=sum(result_total)\n",
    "        print('sum_result',min(result_total))\n",
    "        #--------------------- EXOGENOUS FORECAST\n",
    "        if min(result_total)>0:\n",
    "            #read the dataset where we will add the new line of the exogeneous prediction\n",
    "            if model=='xgb':\n",
    "                dataset_exo = pd.read_csv('final_xgb_exo.csv') \n",
    "            elif model=='rf':\n",
    "                dataset_exo = pd.read_csv('final_rf_exo.csv') \n",
    "            dataset_pd=pd.DataFrame(dataset_exo)\n",
    "\n",
    "            dataset_pd=dataset_pd.replace({dataset_names.columns[26]:{1.02:1}})\n",
    "            dataset_pd=dataset_pd.replace({dataset_names.columns[28]:{1.02:0}})\n",
    "\n",
    "            if time==1:\n",
    "                dataset_pd=dataset_pd.replace({dataset_names.columns[27]:{1.02:dataset_pd.iloc[czech_end,2]}})\n",
    "                dataset_pd=dataset_pd.replace({dataset_names.columns[29]:{1.02:dataset_pd.iloc[czech_end,4]}})\n",
    "                dataset_pd=dataset_pd.replace({dataset_names.columns[30]:{1.02:dataset_pd.iloc[0,5]}})\n",
    "\n",
    "            if time!=1:\n",
    "                dataset_pd=dataset_pd.replace({dataset_names.columns[27]:{1.02:dataset_pd.iloc[20+time,2]}})\n",
    "                dataset_pd=dataset_pd.replace({dataset_names.columns[29]:{1.02:dataset_pd.iloc[len(dataset_pd)-4,4]}})\n",
    "                dataset_pd=dataset_pd.replace({dataset_names.columns[30]:{1.02:dataset_pd.iloc[78+time,5]}})\n",
    "\n",
    "            for var in range(0,no_col-5):\n",
    "                dataset_pd=dataset_pd.replace({dataset_names.columns[var+no_col+6] : {1.02: matrix[vector_zeros[time-2],var]}}) \n",
    "            \n",
    "\n",
    "            dataset_new=dataset_pd\n",
    "            dataset_new.columns=dataset_names.columns\n",
    "            quarter=[]\n",
    "            years=[]\n",
    "            for k in range(19):\n",
    "                quarter=(2021+k)\n",
    "                for i in range(4):\n",
    "                    years.append(quarter)\n",
    "            import numpy\n",
    "            year=years[2:20]\n",
    "            months=[]\n",
    "            quarters=[0]\n",
    "            for k in range(4):\n",
    "                quarter=((k+1)*3)\n",
    "                months.append(quarter)\n",
    "            month=numpy.tile(months,19)\n",
    "            month=month[2:len(year)+2]\n",
    "\n",
    "            #append new values for exogenous forecasts to the dataset\n",
    "            data=np.zeros(no_col_full)\n",
    "            final=[]\n",
    "            for col in range(0,no_col):\n",
    "                data[col]=dataset_new.iloc[len(dataset_new)-1,26+col]   \n",
    "            for col in range(no_col,no_col_full): #-5\n",
    "                data[col]=1.02\n",
    "            dataset_line=pd.DataFrame(data)\n",
    "            dataset_line=dataset_line.T\n",
    "            dataset_line.columns=dataset_pd.columns[1:]\n",
    "            #dataset_line=dataset_line.replace({dataset_names.columns[3]:{0:1}})\n",
    "            #dataset_line=dataset_line.replace({dataset_names.columns[4]:{0:month[vector_zeros[time-2]]}})\n",
    "            #dataset_line=dataset_line.replace({dataset_names.columns[5]:{0:year[vector_zeros[time-2]]}})\n",
    "            final=dataset_new.append(dataset_line,ignore_index=True)\n",
    "            \n",
    "            if model=='xgb':\n",
    "                final.to_csv('final_xgb_exo.csv', index=False) \n",
    "            \n",
    "            if model=='rf':\n",
    "                final.to_csv('final_rf_exo.csv', index=False) \n",
    "            \n",
    "        if model=='xgb':\n",
    "            dataset = pd.read_csv('final.csv')\n",
    "        elif model=='rf':\n",
    "            dataset = pd.read_csv('final.csv') \n",
    "        val=list(range(0,no_col))\n",
    "        column_indices = [val]\n",
    "        new_names = ['index','date', 'day',\t'month', 'year',\t'GDP',\t'CONS_HHS',\t'CONS_GOV',\t'THFK',\t'THK',\t'EXPORT', 'IMPORT',\t'GDP_EA',\t'WAGE',\t'W_EA',\t'L', 'CPI', 'NER', 'CONST_INDEX', 'CON_INDEX', 'IND_INDEX', 'RETAIL_INDEX',\t'SERV_INDEX', 'IR', 'NX']\n",
    "        old_names = dataset.columns[column_indices]\n",
    "        dataset.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "        #get vector of quarters and years to add to the new prediction line\n",
    "\n",
    "        #select values for the variables used in the model\n",
    "        hh=dataset.head()\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        \n",
    "        # if non-random selection:\n",
    "\n",
    "        #values = dataset.values\n",
    "        #train_size=0.6 #0.8\n",
    "        #train=pd.DataFrame()\n",
    "        #test=pd.DataFrame()\n",
    "\n",
    "        #for val in range(0,len(dataset)):\n",
    "        #    start_train=int(val*80)\n",
    "        #    end_train=int(start_train+80*train_size)\n",
    "        #    start_test=int(end_train)\n",
    "        #    end_test=int(start_train+80)\n",
    "        #    train_sample=dataset.iloc[start_train:end_train,:]\n",
    "        #    test_sample=dataset.iloc[start_test:end_test,:]\n",
    "        #    train=pd.concat([train,train_sample])\n",
    "        #    test=pd.concat([test,test_sample])\n",
    "        #test=pd.DataFrame(test)\n",
    "\n",
    "        # how long testing data for Czechia\n",
    "        import numpy as np\n",
    "        testing=test.iloc[:,1].values\n",
    "        occ=np.count_nonzero(testing==0)\n",
    "\n",
    "        if x==1:\n",
    "            X_before=train.iloc[:,0:11].values\n",
    "            X_others=train.iloc[:,13:no_col].values\n",
    "            train_features=np.concatenate((X_before,X_others), axis=1) #features\n",
    "            X_before_test=test.iloc[0:occ,0:11].values #9,11\n",
    "            X_others_test=test.iloc[0:occ,13:no_col].values\n",
    "            test_features=np.concatenate((X_before_test,X_others_test), axis=1) #features\n",
    "        if x!=1:\n",
    "            train_features=train.iloc[:, 0:(no_col)].values\n",
    "            test_features=test.iloc[0:occ, 0:(no_col)].values\n",
    "        \n",
    "        train_labels = train.iloc[:, no_col+6+x-1].values\n",
    "        test_labels = test.iloc[0:occ, no_col+6+x-1].values\n",
    "        labels = dataset.iloc[:, no_col+6+x-1].values\n",
    "\n",
    "        if model=='LSTM':\n",
    "            # reshape input to be 3D as required by LSTM [samples, timesteps, features]\n",
    "            train_features = train_features.reshape((train_features.shape[0], 1, train_features.shape[1]))\n",
    "            test_features = test_features.reshape((test_features.shape[0], 1, test_features.shape[1]))\n",
    "            #print(train_features.shape, train_labels.shape, test_features.shape, test_labels.shape)\n",
    "            #print(test_features.shape[2])\n",
    "\n",
    "        #Running this example prints the shape of the train and test input and output sets\n",
    "        #make exogenous forecast from the randomly selected values and always add future row to get future predictions\n",
    "        if scenario==no_col or scenario==1: \n",
    "            future_row1=dataset.iloc[(len(dataset)-1):(len(dataset)), 0:11].values\n",
    "            future_row2=dataset.iloc[(len(dataset)-1):(len(dataset)), 13:no_col].values\n",
    "            future_row=np.concatenate((future_row1,future_row2), axis=1)\n",
    "            future_features=test_features\n",
    "            future=geek.zeros([len(test_features)+2, no_col-2])\n",
    "\n",
    "        elif scenario!=no_col and scenario!=1: #estimate the remaining models with regards to Export and Import variables\n",
    "            future_row=dataset.iloc[(len(dataset)-1):(len(dataset)), 0:no_col].values\n",
    "            future_features=test_features\n",
    "            future=geek.zeros([len(test_features)+2, no_col])\n",
    "        future_labels=future[0:len(test_labels), 1]*0\n",
    "        future[0:len(test_features),:]=future_features\n",
    "        future[len(test_features),:]=future_row\n",
    "        future=future[1:(len(future)-1),:]\n",
    "\n",
    "        future_labels[0:(len(test_labels)-1)]=test_labels[1:(len(test_labels))]\n",
    "        future_labels[(len(future_labels)-1):len(future_labels)]=labels[len(labels)-1]\n",
    "\n",
    "        # Saving feature names for later use\n",
    "        feature_list= ['index','date', 'day',\t'month', 'year',\t'GDP',\t'CONS_HHS',\t'CONS_GOV',\t'THFK',\t'THK',\t'EXPORT', 'IMPORT',\t'GDP_EA',\t'WAGE',\t'W_EA',\t'L', 'CPI', 'NER', 'CONST_INDEX', 'CON_INDEX', 'IND_INDEX', 'RETAIL_INDEX',\t'SERV_INDEX', 'IR']\n",
    "        baseline_preds = test_features[0:occ,13]\n",
    "\n",
    "        # Baseline errors, and display average baseline error\n",
    "        baseline_errors = abs(baseline_preds - test_labels)\n",
    "\n",
    "        # Import the model we are using\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        # Instantiate model with 1000 decision trees\n",
    "        rf = RandomForestRegressor(n_estimators= 1000) #, random_state=42\n",
    "        xgb = XGBRegressor(importance_type='gain',objective='reg:squarederror',n_estimators= 1000)\n",
    "\n",
    "        # Train the model on training data\n",
    "        rf.fit(train_features, train_labels)\n",
    "        xgb.fit(train_features, train_labels)\n",
    "\n",
    "        # # Use the forest's predict method on the test data\n",
    "        prediction_rf = rf.predict(test_features)\n",
    "        prediction_xgb=xgb.predict(test_features)\n",
    "        prediction_xgb_exo=xgb.predict(future)\n",
    "        prediction_rf_exo=rf.predict(future)\n",
    "        if model=='xgb':\n",
    "            predictions=prediction_xgb \n",
    "            predictions_exo=prediction_xgb_exo\n",
    "        if model=='rf':\n",
    "            predictions=prediction_rf\n",
    "            predictions_exo=prediction_rf_exo\n",
    "\n",
    "        if model=='xgb':\n",
    "            predictions=prediction_xgb \n",
    "            predictions_exo=prediction_xgb_exo\n",
    "        if model=='rf':\n",
    "            predictions=prediction_rf\n",
    "            predictions_exo=prediction_rf_exo\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - test_labels)\n",
    "        # Print out the mean absolute error (mae)\n",
    "        mae=(round(np.mean(errors), 2))\n",
    "        # # Calculate mean absolute percentage error (MAPE)\n",
    "        mape = 100 * abs(errors /(test_labels))\n",
    "        # Calculate and display accuracy\n",
    "        accuracy = 100 - np.mean(mape)\n",
    "        print('Accuracy:', round(accuracy, 2), '%.')\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from math import sqrt\n",
    "        from sklearn.metrics import r2_score\n",
    "\n",
    "        rms = sqrt(mean_squared_error(test_labels, predictions)) #tip: measure for testing and training data, compare difference\n",
    "\n",
    "        # from sklearn.metrics import mean_squared_error, r2_score\n",
    "        model_score =  metrics.r2_score(test_labels, predictions)\n",
    "        r2=(model_score)\n",
    "\n",
    "        # measure the accuracy of the model for all the test values without the last value that enters the endogenous or exogenous prediction\n",
    "        r2_endo=predictions*0\n",
    "        rms_endo = predictions*0\n",
    "        errors_endo = predictions*0\n",
    "        mae_endo = predictions*0\n",
    "        for per in range(0,len(predictions)):\n",
    "            test_labels_endo=test_labels[0:(len(test_labels)-1)]*0\n",
    "            predictions_endo=predictions[0:(len(predictions)-1)]*0\n",
    "                \n",
    "            if per>0:\n",
    "                test_labels_endo[0:(per-1)]=test_labels[0:(per-1)]\n",
    "                predictions_endo[0:(per-1)]=predictions[0:(per-1)]\n",
    "                test_labels_endo[(per):len(test_labels_endo)]=test_labels[(per+1):len(test_labels)]\n",
    "                predictions_endo[per:len(predictions_endo)]=predictions[(per+1):len(predictions)]\n",
    "\n",
    "            if per==0:\n",
    "                test_labels_endo[0:len(test_labels_endo)]=test_labels[1:len(test_labels)]\n",
    "                predictions_endo[0:len(predictions_endo)]=predictions[1:len(predictions)]\n",
    "\n",
    "            r2_endo[per]=metrics.r2_score(test_labels_endo, predictions_endo)\n",
    "            rms_endo[per]=sqrt(mean_squared_error(test_labels_endo, predictions_endo))\n",
    "            errors_endo = abs(predictions_endo - test_labels_endo)\n",
    "            mae_endo[per]=(round(np.mean(errors_endo), 2))\n",
    "\n",
    "        model_score_future= metrics.r2_score(future_labels[0:(len(future_labels)-1)], predictions_exo[0:(len(predictions_exo)-1)])\n",
    "        r2_future=model_score_future\n",
    "        rms_future=sqrt(mean_squared_error(future_labels[0:(len(future_labels)-1)], predictions_exo[0:(len(predictions_exo)-1)]))\n",
    "\n",
    "        print(\"Mean squared error: %.2f\"% mean_squared_error(test_labels, predictions))\n",
    "        mse=(mean_squared_error(test_labels, predictions))\n",
    "        # Explained variance score: 1 is perfect prediction\n",
    "        print('Test Variance score: %.2f' % r2_score(test_labels, predictions))\n",
    "        r2score=(r2_score(test_labels, predictions))\n",
    "\n",
    "        score=rf.score(test_features, test_labels)\n",
    "\n",
    "        #---------------------ENDOGENOUS FORECAST\n",
    "        #---------------------Append forecast to original dataset\n",
    "\n",
    "        #read the dataset where you want to add the endogenous forecast\n",
    "        if model=='xgb':\n",
    "            dataset1 = pd.read_csv('final_xgb.csv')\n",
    "        if model=='rf':\n",
    "            dataset1 = pd.read_csv('final_rf.csv')\n",
    "        dataset_endo=dataset1.values\n",
    "        dfObj = pd.DataFrame(dataset_endo)\n",
    "\n",
    "        import datetime\n",
    "        test=pd.DataFrame(test)\n",
    "        together=test.iloc[:,1:no_col+1].values\n",
    "        inv_transformed=scaler.inverse_transform(together)\n",
    "        inv_transformed=pd.DataFrame(inv_transformed)\n",
    "        \n",
    "        country_ind = inv_transformed.iloc[:,0]\n",
    "        months = inv_transformed.iloc[:,3]\n",
    "        days = inv_transformed.iloc[:,2] \n",
    "        years = round(inv_transformed.iloc[:,4]) \n",
    "        test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "        test_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]\n",
    "\n",
    "        #this line is important for the exogenous forecast\n",
    "        #only if all the variables are now predicted for the future add 1 to the vector called vector_zeros so you can add new line to the \"matrix\" filled with future values.\n",
    "        if min(result_total)>0:\n",
    "            vector_zeros[time-1]=vector_zeros[time-2]+1\n",
    "            for k in range(time,delka-1):\n",
    "                vector_zeros[k]=vector_zeros[k-1]\n",
    "            result=0\n",
    "            sum_result=0\n",
    "            result_total=np.zeros(no_col-5)\n",
    "\n",
    "        if model=='xgb':\n",
    "            result=prediction_xgb_exo[len(prediction_xgb_exo)-1]\n",
    "        if model=='rf':\n",
    "            result=prediction_rf_exo[len(prediction_rf_exo)-1]\n",
    "        result_total[x-1]=result\n",
    "        matrix[vector_zeros[time-1],x-1]=result\n",
    "        dates=test_dates\n",
    "        rms_endo=rms_endo\n",
    "        rms=rms\n",
    "        mae=mae_endo\n",
    "        mape=mape\n",
    "        acc=accuracy\n",
    "        mse=mse \n",
    "        r2=r2\n",
    "        r2score=r2score\n",
    "        r2_endo=r2_endo\n",
    "        result_endo=predictions\n",
    "        #append the endogenous prediction values to the original dataset1\n",
    "        blocks = np.array([result_endo[0:occ]]).T\n",
    "        blocks=pd.DataFrame(blocks)\n",
    "\n",
    "        result_endo_pred[x-1]=blocks.iloc[:,0]\n",
    "    \n",
    "    result_endo_pred1=pd.concat([result_endo_pred1,result_endo_pred],ignore_index=True)\n",
    "\n",
    "    values=pd.DataFrame()\n",
    "    ran=[5,4,3,2,1]\n",
    "    for val in ran:\n",
    "        hod=ran.index(val)\n",
    "        values[hod]=test.iloc[0:occ,val]\n",
    "    values_all=values_all.append(values)\n",
    "\n",
    "dataframe_endo=pd.DataFrame()\n",
    "values_all=values_all.reset_index()\n",
    "dataframe_endo=pd.concat([values_all,result_endo_pred1],axis=1,ignore_index=True)\n",
    "\n",
    "colum_all=[]\n",
    "colum_all=['index']\n",
    "colum_all.append(dataset1.columns[5])\n",
    "colum_all.append(dataset1.columns[4])\n",
    "colum_all.append(dataset1.columns[3])\n",
    "colum_all.append(dataset1.columns[2])\n",
    "colum_all.append(dataset1.columns[1])\n",
    "for cn in range(6,len(dataframe_endo.columns)):\n",
    "    colum_endo=dataset1.columns[cn]\n",
    "    colum_all.append(colum_endo)\n",
    "dataframe_endo.columns=colum_all\n",
    "dataset1=pd.concat([dataset1,dataframe_endo],ignore_index=True)\n",
    "\n",
    "cut=(dataset1.iloc[:,7])\n",
    "cut=cut.dropna()\n",
    "cut=len(cut)\n",
    "dataset1=dataset1.iloc[0:int(cut),:]\n",
    "print(dataset1.tail(15))\n",
    "if model=='rf':\n",
    "    dataset1.to_csv('final_rf.csv', index=False)\n",
    "if model=='xgb':\n",
    "    dataset1.to_csv('final_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDP GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform matrix back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in ('final_rf.csv','final_rf_exo.csv','final_xgb.csv','final_xgb_exo.csv'):\n",
    "    transform_back=pd.read_csv(file)\n",
    "    transform_back=transform_back.values[:,1:26]\n",
    "    inv_transformed=scaler.inverse_transform(transform_back)\n",
    "    inv_transformed=pd.DataFrame(inv_transformed)\n",
    "    inv_transformed.iloc[:,4]=round(inv_transformed.iloc[:,4])\n",
    "    inv_transformed.iloc[:,3]=round(inv_transformed.iloc[:,3])\n",
    "    column_indices=inv_transformed.columns\n",
    "    \n",
    "    new_names = ['index','date', 'day',\t'month', 'year',\t'GDP',\t'CONS_HHS',\t'CONS_GOV',\t'THFK',\t'THK',\t'EXPORT', 'IMPORT',\t'GDP_EA',\t'WAGE',\t'W_EA',\t'L', 'CPI', 'NER', 'CONST_INDEX', 'CON_INDEX', 'IND_INDEX', 'RETAIL_INDEX',\t'SERV_INDEX', 'IR', 'NX']\n",
    "    old_names = inv_transformed.columns#[column_indices]\n",
    "    inv_transformed.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    \n",
    "    val=list(range(0,len(inv_transformed)))\n",
    "    inv_transformed['values']=val\n",
    "    #print(inv_transformed.iloc[-1:-5,10:15])\n",
    "\n",
    "    transform_back2=pd.read_csv(file)\n",
    "    transform_back2=transform_back2.values[:,26:51]\n",
    "    inv_transformed2=scaler.inverse_transform(transform_back2)\n",
    "    inv_transformed2=pd.DataFrame(inv_transformed2)\n",
    "    inv_transformed2.iloc[:,4]=round(inv_transformed2.iloc[:,4])\n",
    "    column_indices=inv_transformed2.columns\n",
    "    new_names = ['indext','datet', 'dayt',\t'montht', 'yeart',\t'GDPt',\t'CONS_HHSt',\t'CONS_GOVt',\t'THFKt',\t'THKt',\t'EXPORTt', 'IMPORTt',\t'GDP_EAt',\t'WAGEt',\t'W_EAt',\t'Lt', 'CPIt', 'NERt', 'CONST_INDEXt', 'CON_INDEXt', 'IND_INDEXt', 'RETAIL_INDEXt',\t'SERV_INDEXt', 'IRt', 'NXt']\n",
    "    old_names = inv_transformed2.columns#[column_indices]\n",
    "    inv_transformed2.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    val=list(range(0,len(inv_transformed2)))\n",
    "    inv_transformed2.insert(0,'values',val)\n",
    "    \n",
    "    inv_transformed_total=pd.merge(inv_transformed,inv_transformed2,on=\"values\")\n",
    "    inv_transformed_total=inv_transformed_total.drop(['values','indext','datet','dayt','montht','yeart'],axis=1)\n",
    "    d={'day':inv_transformed_total.iloc[:,2],\t'month':inv_transformed_total.iloc[:,3],\t'year':inv_transformed_total.iloc[:,4],\t'dy':inv_transformed_total.iloc[:,5],\t'dc':inv_transformed_total.iloc[:,6],\t'dx':inv_transformed_total.iloc[:,10],\t'dm':inv_transformed_total.iloc[:,11],\t'dR':inv_transformed_total.iloc[:,23],\t'dConSen_bal':inv_transformed_total.iloc[:,19],\t'dPMI_manuf':inv_transformed_total.iloc[:,20],\t'dInd_conf':inv_transformed_total.iloc[:,20],\t'dConst_conf':inv_transformed_total.iloc[:,18],\t'dw':inv_transformed_total.iloc[:,13], 'yt':inv_transformed_total.iloc[:,25],\t'ct':inv_transformed_total.iloc[:,26],\t'yeat':inv_transformed_total.iloc[:,32],\t'xt':inv_transformed_total.iloc[:,30],\t'mt':inv_transformed_total.iloc[:,31],\t'Rt':inv_transformed_total.iloc[:,43],\t'ConSen_balt':inv_transformed_total.iloc[:,39],\t'PMI_manuft':inv_transformed_total.iloc[:,40],\t'Ind_conft':inv_transformed_total.iloc[:,40],\t'Const_conft':inv_transformed_total.iloc[:,38],\t'wt':inv_transformed_total.iloc[:,33], 'dyea':inv_transformed_total.iloc[:,11]}\n",
    "    inv_transformed_compared=pd.DataFrame(data=d)\n",
    "    if file=='final_rf.csv':\n",
    "        inv_transformed_compared.to_csv('rf_transform_back.csv',index=False)\n",
    "    if file=='final_rf_exo.csv':\n",
    "        inv_transformed_compared.to_csv('rf_exo_transform_back.csv',index=False)\n",
    "    if file=='final_xgb.csv':\n",
    "        inv_transformed_compared.to_csv('xgb_transform_back.csv',index=False)\n",
    "    if file=='final_xgb_exo.csv':\n",
    "        inv_transformed_compared.to_csv('xgb_exo_transform_back.csv',index=False)\n",
    "    print(len(inv_transformed_compared))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAPHS FOR THE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevengawthorpe/.pyenv/versions/my-env/lib/python3.8/site-packages/pandas/core/indexes/base.py:3941: FutureWarning:\n",
      "\n",
      "Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model='rf' #type 'rf' if you want the variable importance graphs for the random forest model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "import plotly\n",
    "import chart_studio\n",
    "chart_studio.tools.set_credentials_file(username='sgawthor', api_key='wa3B5oJOSFTqcBuopZI2')\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import numpy as geek\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#set subplots\n",
    "for x in range(1,5): #get results for the first four variables\n",
    "    scenario=x\n",
    "    if model=='xgb':\n",
    "        dataset = pd.read_csv('xgb_exo_transform_back.csv')\n",
    "    if model=='rf':\n",
    "        dataset = pd.read_csv('rf_exo_transform_back.csv')\n",
    "    #1=y\n",
    "    hh=dataset.head()\n",
    "    X = dataset.iloc[:, 0:14].values #25\n",
    "    y = dataset.iloc[:, 14].values\n",
    "\n",
    "    #2=c\n",
    "    yc=dataset.iloc[:, 15].values\n",
    "    Xc=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #3=yea\n",
    "    yea=dataset.iloc[:, 16].values\n",
    "    Xea=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #4=x\n",
    "    yx=dataset.iloc[:, 17].values\n",
    "    Xx=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #5=m\n",
    "    ym=dataset.iloc[:, 18].valuesdataset = pd.read_csv('final.csv') #GDP_tree_VARYoY_Yrf01.csv\n",
    "\n",
    "val=list(range(0,no_col))\n",
    "column_indices = [val]\n",
    "new_names = ['index','index','date', 'day',\t'month', 'year',\t'GDP',\t'CONS_HHS',\t'CONS_GOV',\t'THFK',\t'THK',\t'EXPORT', 'IMPORT',\t'GDP_EA',\t'WAGE',\t'W_EA',\t'L', 'CPI', 'NER', 'CONST_INDEX', 'CON_INDEX', 'IND_INDEX', 'RETAIL_INDEX',\t'SERV_INDEX', 'IR', 'NX']\n",
    "old_names = dataset.columns[column_indices]\n",
    "dataset.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "#get vector of quarters and years to add to the new prediction line\n",
    "\n",
    "#select values for the variables used in the model\n",
    "hh=dataset.head()\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_size=0.8 #0.8\n",
    "train=pd.DataFrame()\n",
    "test=pd.DataFrame()\n",
    "\n",
    "import random\n",
    "random_train=[]\n",
    "random_test=[]\n",
    "train_bulk=[]\n",
    "test_bulk=[]\n",
    "\n",
    "train_value=int(80*train_size)\n",
    "train_bulk=random.sample(range(79), train_value)\n",
    "train_bulk=np.sort(train_bulk)\n",
    "vector=list(range(0,80))\n",
    "\n",
    "for j in range(0,79):\n",
    "    if j not in train_bulk:\n",
    "        test_bulk.append(j)\n",
    "celek=int(len(dataset)/80)+1\n",
    "for val in range(0,celek):\n",
    "    val=int(val)\n",
    "    vall=val*80+80\n",
    "    if vall<len(dataset):\n",
    "        for v in train_bulk:\n",
    "            train_sample=dataset.iloc[v+79*val,:]\n",
    "            train_sample=pd.DataFrame(train_sample)\n",
    "            train_sample=train_sample.T\n",
    "            train=train.append(train_sample)\n",
    "            #train=pd.concat([train,train_sample])\n",
    "        for h in test_bulk:\n",
    "            test_sample=dataset.iloc[h+79*val,:]\n",
    "            test_sample=pd.DataFrame(test_sample)\n",
    "            test_sample=test_sample.T\n",
    "            test=test.append(test_sample)\n",
    "    Xm=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #6=R\n",
    "    yR=dataset.iloc[:, 19].values\n",
    "    XR=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #7=ConSen_bal\n",
    "    yCon=dataset.iloc[:, 20].values\n",
    "    XCon=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #8=PMI_manuf\n",
    "    yPMI_manuf=dataset.iloc[:, 21].values\n",
    "    XPMI_manuf=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #9=Ind_conf\n",
    "    yInd_conf=dataset.iloc[:, 22].values\n",
    "    XInd_conf=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #10=Const_conf\n",
    "    yConst_conf=dataset.iloc[:, 23].values\n",
    "    XConst_conf=dataset.iloc[:, 0:14].values\n",
    "\n",
    "    #11=w\n",
    "    yw=dataset.iloc[:, 24].values\n",
    "    Xw=dataset.iloc[:, 0:14].values\n",
    "    \n",
    "\n",
    "    if scenario==1:\n",
    "        name='dy'\n",
    "        features=X\n",
    "        labels=y\n",
    "    elif scenario==2:\n",
    "        name='dc'\n",
    "        features=Xc\n",
    "        labels=yc\n",
    "    elif scenario==3:\n",
    "        name='dyea'\n",
    "        features=Xea\n",
    "        labels=yea\n",
    "    elif scenario==4:\n",
    "        name='dx'\n",
    "        features=Xx\n",
    "        labels=yx\n",
    "    elif scenario==5:\n",
    "        name='dm'\n",
    "        features=Xm\n",
    "        labels=ym\n",
    "    elif scenario==6:\n",
    "        name='dR'\n",
    "        features=XR\n",
    "        labels=yR\n",
    "    elif scenario==7:\n",
    "        name='dConSen_bal'\n",
    "        features=XCon\n",
    "        labels=yCon\n",
    "    elif scenario==8:\n",
    "        name='dPMI_manuf'\n",
    "        features=XPMI_manuf\n",
    "        labels=yPMI_manuf\n",
    "    elif scenario==9:\n",
    "        name='dInd_conf'\n",
    "        features=XInd_conf\n",
    "        labels=yInd_conf\n",
    "    elif scenario==10:\n",
    "        name='dConst_conf'\n",
    "        features=XConst_conf\n",
    "        labels=yConst_conf\n",
    "    elif scenario==11:\n",
    "        name='dw'\n",
    "        features=Xw\n",
    "        labels=yw    \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    " \n",
    "    # Saving feature names for later use\n",
    "    feature_list=['day', 'month', 'year', 'Ldy', 'dc', 'dyea', 'dx',\t'dm', 'dR', 'dConSen_bal', 'dPMI_manuf', 'dInd_conf', 'dConst_conf', 'dw']\n",
    "    baseline_preds = test_features[:, feature_list.index('dyea')]\n",
    "\n",
    "    # Baseline errors, and display average baseline error\n",
    "    baseline_errors = abs(baseline_preds - test_labels)\n",
    "\n",
    "    # Import the model we are using\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    rf = RandomForestRegressor(n_estimators= 1000, random_state=42)\n",
    "    xgb = XGBRegressor()\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels)\n",
    "    xgb.fit(train_features, train_labels)\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions_xgb=xgb.predict(test_features)\n",
    "    predictions_rf=rf.predict(test_features)\n",
    "    if model=='rf':\n",
    "        predictions=predictions_rf\n",
    "    if model=='xgb':\n",
    "        predictions=predictions_xgb\n",
    "\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - test_labels)\n",
    "   \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * abs(errors /(test_labels))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape) #one outlier prediction can bias it\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    rms = sqrt(mean_squared_error(test_labels, predictions)) #rms = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "    # from sklearn.metrics import mean_squared_error, r2_score\n",
    "    model_score =  metrics.r2_score(test_labels, predictions)\n",
    "\n",
    "    score=rf.score(test_features, test_labels)\n",
    "\n",
    "    # #--------------------Variable importance\n",
    "    # Get numerical feature importances\n",
    "    importances = list(rf.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    " #------------------VISUALIZATION\n",
    " # FIRST VISUALIZE THE VARIABLES' IMPORTANCE FOR EACH MODEL\n",
    "\n",
    "#   bar plot of the feature importances \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #matplotlib inline\n",
    "    # Set the style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    fig1, ax = plt.subplots(facecolor=(1,1,1)) \n",
    "     # list of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "    # Make a bar chart\n",
    "    plt.bar(x_values, importances, orientation = 'vertical')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('linen')\n",
    "    # Tick labels for x axis\n",
    "    feature_list1=['day', 'month', 'year', 'y', 'c', 'yea', 'x',\t'm', 'R', 'HHs conf.', 'PMI manuf.', 'Ind conf.', 'Const conf.', 'w']\n",
    "    plt.xticks(x_values, feature_list1, rotation='vertical')\n",
    "    # Axis labels and title\n",
    "    plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances')\n",
    "    fig1 = plt.gcf()\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Write graph to a png file\n",
    "    var_name = 'Forest_importance_Y'\n",
    "    var_name += name\n",
    "    var_name += '.png'\n",
    "    fig1.savefig(var_name, bbox_inches='tight', dpi=100, facecolor=fig1.get_facecolor())# edgecolor='none'\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='rf' #type 'rf' if you want the variable importance graphs for the random forest model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "import plotly\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import numpy as geek\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# SECOND VISUALIZE THE PREDICTIONS FROM THE PREVIOUSLY ESTIMATED MODELS\n",
    "for x in range(1,5):\n",
    "    scenario=x\n",
    " # Read the csv files with the obtained values\n",
    "    dataset_base=pd.read_csv('final.csv')\n",
    "    dataset1 = pd.read_csv('rf_transform_back.csv') #rf\n",
    "    dataset1_exo = pd.read_csv('rf_exo_transform_back.csv') #rf\n",
    "\n",
    "    dataset3 = pd.read_csv('xgb_transform_back.csv') #xgb\n",
    "    dataset3_exo = pd.read_csv('xgb_exo_transform_back.csv') #xgb\n",
    "\n",
    "    dataset_names=['day',\t'month',\t'year',\t'dy',\t'dc',\t'dx',\t'dm',\t'dR',\t'dConSen_bal',\t'dPMI_manuf',\t'dInd_conf',\t'dConst_conf',\t'dw', 'yt',\t'ct',\t'yeat',\t'xt',\t'mt',\t'Rt',\t'ConSen_balt',\t'PMI_manuft',\t'Ind_conft',\t'Const_conft',\t'wt', 'dyea']\n",
    "    dataset_pd=dataset1_exo.iloc[0:len(dataset1_exo), 0:25].values #55\n",
    "    #dataset_pd[0:len(dataset1_exo),5:24]=dataset1_exo.iloc[0:len(dataset1_exo), 6:25].values\n",
    "    dataset_pd1=dataset1.iloc[len(dataset1_exo):len(dataset1), 0:25].values\n",
    "    #dataset_pd1[:,5:24]=dataset1.iloc[len(dataset1_exo):len(dataset1), 6:25].values\n",
    "    dataset_pd_xgb=dataset3_exo.iloc[0:len(dataset3_exo), 0:25].values #55\n",
    "    #dataset_pd_xgb[:,5:24]=dataset3_exo.iloc[0:len(dataset3_exo), 6:25].values\n",
    "    dataset_pd1_xgb=dataset3.iloc[len(dataset3_exo):len(dataset3), 0:25].values\n",
    "    #dataset_pd1_xgb[:,5:24]=dataset3.iloc[len(dataset3_exo):len(dataset3), 6:25].values\n",
    "\n",
    "    #create dataframes\n",
    "    dfObj = pd.DataFrame(dataset_pd, columns=dataset_names)\n",
    "    dfObj1=pd.DataFrame(dataset_pd1,columns=dataset_names)\n",
    "    dfObj=dfObj.append(dfObj1) #append the endogenous forecasts to the datafile containing also exogenous forecasts\n",
    "    dfObj_xgb = pd.DataFrame(dataset_pd_xgb, columns=dataset_names)\n",
    "    dfObj1_xgb=pd.DataFrame(dataset_pd1_xgb,columns=dataset_names)\n",
    "    dfObj_xgb=dfObj_xgb.append(dfObj1_xgb) #append the endogenous forecasts to the datafile containing also exogenous forecasts\n",
    "\n",
    "    dfObj_xgb.to_csv('GDP_xgb_blog.csv', index=False)\n",
    "    dataset4 = pd.read_csv('GDP_xgb_blog.csv')\n",
    "\n",
    "    dfObj.to_csv('GDP_rf_blog.csv', index=False)\n",
    "    dataset2 = pd.read_csv('GDP_rf_blog.csv') \n",
    "\n",
    "    #reat csv with the Ministry of Finance and the Czech National Bank predictions\n",
    "    dataset_mfcnb = pd.read_csv('cnb_mf_predikce.csv')\n",
    "\n",
    "    #GRAPH\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Use datetime for creating date objects for plotting\n",
    "    import datetime\n",
    "    # Dates of training values\n",
    "    months = dataset2.iloc[0:len(dataset2), 1].values\n",
    "    days = dataset2.iloc[0:len(dataset2), 0].values\n",
    "    years = dataset2.iloc[0:len(dataset2), 2].values\n",
    "    months_xgb = dataset4.iloc[0:len(dataset4), 1].values\n",
    "    days_xgb = dataset4.iloc[0:len(dataset4), 0].values\n",
    "    years_xgb = dataset4.iloc[0:len(dataset4), 2].values\n",
    "    months_mfcnb = dataset_mfcnb.iloc[0:54, 1].values\n",
    "    days_mfcnb = dataset_mfcnb.iloc[0:54, 0].values\n",
    "    years_mfcnb = dataset_mfcnb.iloc[0:54, 2].values\n",
    "    # List and then convert to datetime object\n",
    "    dates_endo = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "    dates_endo = [datetime.datetime.strptime(date_endo, '%Y-%m-%d') for date_endo in dates_endo]\n",
    "\n",
    "    dates_endo_xgb = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years_xgb, months_xgb, days_xgb)]\n",
    "    dates_endo_xgb = [datetime.datetime.strptime(date_endo, '%Y-%m-%d') for date_endo in dates_endo_xgb]\n",
    "\n",
    "    dates_endo_mfcnb = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years_mfcnb, months_mfcnb, days_mfcnb)]\n",
    "    dates_endo_mfcnb = [datetime.datetime.strptime(date_endo, '%Y-%m-%d') for date_endo in dates_endo_mfcnb]\n",
    "\n",
    "    # Dataframe with true values and date\n",
    "    true_data_endo = pd.DataFrame(data = {'date_endo': dates_endo[0:79], 'actual_endo': dataset2.iloc[0:79, 2+x].values})\n",
    "    # Dataframe with exogenous predictions\n",
    "    predictions_data = pd.DataFrame(data = {'date_endo': dates_endo[len(dataset1_exo):len(dataset1)], 'prediction': dataset2.iloc[len(dataset1_exo):len(dataset1), 2+x].values})\n",
    "    predictions_data_xgb = pd.DataFrame(data = {'date_endo': dates_endo_xgb[len(dataset3_exo):len(dataset3)], 'prediction': dataset4.iloc[len(dataset3_exo):len(dataset3), 2+x].values})\n",
    "    # Dates of predictions\n",
    "\n",
    "    quarter=[]\n",
    "    years=[]\n",
    "    for k in range(19):\n",
    "        quarter=(2021+k)\n",
    "        for i in range(4):\n",
    "            years.append(quarter)\n",
    "    import numpy\n",
    "    year=years[1:22]\n",
    "\n",
    "    months = dataset2.iloc[len(dataset1_exo):len(dataset1), 1].values\n",
    "    days = dataset2.iloc[len(dataset1_exo):len(dataset1), 0].values\n",
    "    years = dataset2.iloc[len(dataset1_exo):len(dataset1), 2].values\n",
    "    months_exo = dataset2.iloc[len(dataset_base)-1:len(dataset1_exo), 1].values\n",
    "    days_exo = dataset2.iloc[len(dataset_base)-1:len(dataset1_exo), 0].values\n",
    "    years_exo = year[0:len(days_exo)]\n",
    "    months_xgb = dataset4.iloc[len(dataset3_exo):len(dataset3), 1].values\n",
    "    days_xgb = dataset4.iloc[len(dataset3_exo):len(dataset3), 0].values\n",
    "    years_xgb = dataset4.iloc[len(dataset3_exo):len(dataset3), 2].values\n",
    "    # Column of dates\n",
    "    test_dates_endo = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "    test_dates_exo = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years_exo, months_exo, days_exo)]\n",
    "    test_dates_endo_xgb = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years_xgb, months_xgb, days_xgb)]\n",
    "    # Convert to datetime objects\n",
    "    test_dates_endo = [datetime.datetime.strptime(date_endo, '%Y-%m-%d') for date_endo in test_dates_endo]\n",
    "    test_dates_endo_xgb = [datetime.datetime.strptime(date_endo, '%Y-%m-%d') for date_endo in test_dates_endo_xgb]\n",
    "    test_dates_exo = [datetime.datetime.strptime(date_exo, '%Y-%m-%d') for date_exo in test_dates_exo]\n",
    "    \n",
    "    # Dataframe with predictions and dates\n",
    "    predictions_data_endo = pd.DataFrame(data = {'date_endo': test_dates_endo, 'prediction_endo': dataset2.iloc[len(dataset1_exo):len(dataset1), 2+x].values})\n",
    "    predictions_data_endo_xgb = pd.DataFrame(data = {'date_endo': test_dates_endo_xgb, 'prediction_endo': dataset4.iloc[len(dataset3_exo):len(dataset3), 2+x].values})\n",
    "    variab = ['Gross domestic product', 'Consumption', 'Export', 'Import', 'PRIBOR', 'Consumption sentiment', 'PMI manufacturing', 'Construction confidence', 'Industrial confidence', 'Wage', 'GDP EA']\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import datetime\n",
    "\n",
    "\n",
    "#  FUTURE FORECAST OBTAINED FROM RF AND XGB AS ONE VALUE \n",
    "    future_gdp=dataset2.iloc[(len(dataset_base)-1):len(dataset1_exo),3].values\n",
    "    future_con=dataset2.iloc[(len(dataset_base)-1):len(dataset1_exo),4].values\n",
    "    future_exp=dataset2.iloc[(len(dataset_base)-1):len(dataset1_exo),5].values\n",
    "    future_imp=dataset2.iloc[(len(dataset_base)-1):len(dataset1_exo),6].values\n",
    "    \n",
    "    overlapping = 0.150\n",
    "    fig = px.line(true_data_endo, x='date_endo', y='actual_endo', labels={\"actual_endo\":\"\", \"date_endo\":\"years\"}, title=variab[x-1])\n",
    "    fig.update_layout(title={'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'})\n",
    "    fig.update_traces(line_color='black')\n",
    "\n",
    "\n",
    "    new_years=[]\n",
    "    #THE GDP GRAPH\n",
    "    if scenario==1: \n",
    "        ex_forecast= pd.DataFrame(data = {'date_endo': test_dates_exo, 'prediction': future_gdp})\n",
    "        fig.add_trace(go.Scatter(x=ex_forecast['date_endo'], y=ex_forecast['prediction'], mode = 'lines', marker=dict(color='crimson'), showlegend=False))#, row=x,col=1)\n",
    "        fig.add_trace(go.Scatter(name=\"XGB_gdp\",x=predictions_data_endo_xgb['date_endo'], y=predictions_data_endo_xgb['prediction_endo'], mode = 'markers', marker=dict(color='rgba(128,0,128, 0.8)', size=7,line=dict(color='darkblue', width=1))))#, row=x,col=1) #red\n",
    "        fig.add_trace(go.Scatter(name=\"RF_gdp\",x=predictions_data_endo['date_endo'], y=predictions_data_endo['prediction_endo'], mode = 'markers', marker=dict(color='rgba(255,192,203, 0.8)',size=7, line=dict(color='darkblue', width=1))))#, row=x,col=1)\n",
    "\n",
    "            \n",
    "    #THE CONSUMPTION GRAPH\n",
    "    if scenario==2:\n",
    "        fig.add_trace(go.Scatter(name=\"XGB_con\",x=predictions_data_endo_xgb['date_endo'], y=predictions_data_endo_xgb['prediction_endo'], mode = 'markers', marker=dict(color='rgba(128,0,128, 0.7)', size=7,line=dict(color='darkblue', width=1))))#, row=x,col=1)\n",
    "        fig.add_trace(go.Scatter(name=\"RF_con\",x=predictions_data_endo['date_endo'], y=predictions_data_endo['prediction_endo'], mode = 'markers', marker=dict(color='rgba(255,192,203, 0.7)',size=7, line=dict(color='darkblue', width=1))))#, row=x,col=1)\n",
    "        #ex_forecast= pd.DataFrame(data = {'date_endo': test_dates_exo, 'prediction': future_con})\n",
    "        #fig.add_trace(go.Scatter(x=ex_forecast['date_endo'], y=ex_forecast['prediction'], mode = 'lines', marker=dict(color='crimson'), showlegend=False))#, row=x,col=1)\n",
    "\n",
    "    #THE EXPORT GRAPH\n",
    "    if scenario==3:\n",
    "        fig.add_trace(go.Scatter(name=\"XGB_ex\",x=predictions_data_endo_xgb['date_endo'], y=predictions_data_endo_xgb['prediction_endo'], mode = 'markers', marker=dict(color='rgba(128,0,128, 0.7)', size=7,line=dict(color='darkblue', width=1))))#, row=x,col=1)\n",
    "        fig.add_trace(go.Scatter(name=\"RF_ex\",x=predictions_data_endo['date_endo'], y=predictions_data_endo['prediction_endo'], mode = 'markers', marker=dict(color='rgba(255,192,203, 0.7)',size=7, line=dict(color='darkblue', width=1))))#, row=x,col=1)\n",
    "        #ex_forecast= pd.DataFrame(data = {'date_endo': test_dates_exo, 'prediction': future_exp})\n",
    "        #fig.add_trace(go.Scatter(x=ex_forecast['date_endo'], y=ex_forecast['prediction'], mode = 'lines', marker=dict(color='crimson'), showlegend=False))#, row=x,col=1)\n",
    "\n",
    "    #THE IMPORT GRAPH\n",
    "    if scenario==4:\n",
    "        fig.add_trace(go.Scatter(name=\"XGB_im\",x=predictions_data_endo_xgb['date_endo'], y=predictions_data_endo_xgb['prediction_endo'], mode = 'markers', marker=dict(color='rgba(128,0,128, 0.7)', size=7,line=dict(color='darkblue', width=1))))#, row=x,col=1)\n",
    "        fig.add_trace(go.Scatter(name=\"RF_im\",x=predictions_data_endo['date_endo'], y=predictions_data_endo['prediction_endo'], mode = 'markers', marker=dict(color='rgba(255,192,203, 0.7)',size=7, line=dict(color='darkblue', width=1))))#, row=x,col=1)\n",
    "        #ex_forecast= pd.DataFrame(data = {'date_endo': test_dates_exo, 'prediction': future_imp})\n",
    "        #fig.add_trace(go.Scatter(x=ex_forecast['date_endo'], y=ex_forecast['prediction'], mode = 'lines', marker=dict(color='crimson'), showlegend=False))#, row=x,col=1)\n",
    "\n",
    "\n",
    "    fig.update_layout(legend=dict(orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.8,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "        ))\n",
    "\n",
    "    fig.update_xaxes(rangeslider_visible=True) \n",
    "\n",
    "    #if scenario==1:\n",
    "    #    py.plot(fig, filename='PANEL_GDP', auto_open=True)\n",
    "    #if scenario==2:\n",
    "    #    py.plot(fig, filename='PANEL_C', auto_open=True)\n",
    "    #if scenario==3:\n",
    "    #    py.plot(fig, filename='PANEL_X', auto_open=True)\n",
    "    #if scenario==4:\n",
    "    #    py.plot(fig, filename='PANEL_M', auto_open=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dde5c1e12c4df2ec3da3c94d20b96b8a297692105bdf5402094b2e4baff73d03"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('my-env': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "dde5c1e12c4df2ec3da3c94d20b96b8a297692105bdf5402094b2e4baff73d03"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
